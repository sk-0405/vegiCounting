{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vesi-count_pytorch.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeWTK7JrrFuN"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "import xml.etree.ElementTree as ET \n",
        "import cv2\n",
        "\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torch.utils.data import TensorDataset\n",
        "import os\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/***')\n",
        "\n",
        "#pathの指定(colab_frcnn-main直下まで)\n",
        "path='/content/drive/MyDrive/***'\n",
        "bdd_xml=path+\"/***\"\n",
        "bdd_img=path+\"/***\"\n",
        "test_path=path+\"/***\"\n",
        "#datasetのクラス指定\n",
        "dataset_class=['tomato', 'iceplant', 'dried persimmon']\n",
        "#表示したいラベルの色の指定\n",
        "#注意！！一番最初は背景クラスを示すので(0,0,0)にする(それ以外は自由)\n",
        "colors= ((0,0,0),(255,0,0),(0,255,0),(0,0,255))\n",
        "\n",
        "#ハイパーパラメータの指定\n",
        "epochs=10\n",
        "batch_size=1\n",
        "scale=720 #画像のスケール設定(縦の大きさを入力)\n",
        "\n",
        "\n",
        "#labeling Toolで作成した各画像のxmlファイルをlistにする\n",
        "class xml2list(object):\n",
        "\n",
        "    def __init__(self, classes):\n",
        "        self.classes = classes\n",
        "\n",
        "    def __call__(self, xml_path):\n",
        "\n",
        "        ret = []\n",
        "        xml = ET.parse(xml_path).getroot()\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        zz=0\n",
        "\n",
        "        for zz,obj in enumerate(xml.iter('object')):\n",
        "            label = obj.find('name').text\n",
        "\n",
        "            ##指定クラスのみ\n",
        "\n",
        "            if label in self.classes :\n",
        "                bndbox = obj.find('bndbox')\n",
        "                xmin = int(bndbox.find('xmin').text)\n",
        "                ymin = int(bndbox.find('ymin').text)\n",
        "                xmax = int(bndbox.find('xmax').text)\n",
        "                ymax = int(bndbox.find('ymax').text)\n",
        "                boxes.append([xmin, ymin, xmax, ymax])\n",
        "                labels.append(self.classes.index(label))\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "        num_objs = zz +1\n",
        "\n",
        "        anno = {'bboxes':boxes, 'labels':labels}\n",
        "\n",
        "        return anno,num_objs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUSr2dzorFo5"
      },
      "source": [
        "#画像の読み込み\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "\n",
        "        def __init__(self,image_dir,xml_paths,scale,classes):\n",
        "\n",
        "            super().__init__()\n",
        "            self.image_dir = image_dir\n",
        "            self.xml_paths = xml_paths\n",
        "            self.image_ids = sorted(glob('{}/*'.format(xml_paths)))\n",
        "            self.scale=scale\n",
        "            self.classes=classes\n",
        "\n",
        "        def __getitem__(self, index):\n",
        "\n",
        "            transform = transforms.Compose([\n",
        "                                            transforms.ToTensor()\n",
        "            ])\n",
        "\n",
        "            # 入力画像の読み込み\n",
        "            image_id=self.image_ids[index].split(\"/\")[-1].split(\".\")[0]\n",
        "            image = Image.open(f\"{self.image_dir}/{image_id}.jpg\")\n",
        "\n",
        "\n",
        "            #画像のスケール変換\n",
        "            t_scale_tate=self.scale ##目標のスケール(縦)\n",
        "            #縮小比を計算\n",
        "            ratio=t_scale_tate/image.size[1]\n",
        "            ##目標横スケールを計算\n",
        "            t_scale_yoko=image.size[0]*ratio\n",
        "            t_scale_yoko=int(t_scale_yoko)\n",
        "\n",
        "            # print('縮小前:',image.size)\n",
        "            # print('縮小率:',ratio)\n",
        "            #リサイズ\n",
        "            image = image.resize((t_scale_yoko,t_scale_tate))\n",
        "            # print('縮小後:',image.size)\n",
        "\n",
        "            image = transform(image)\n",
        "\n",
        "            transform_anno = xml2list(self.classes)\n",
        "            path_xml=f'{self.xml_paths}/{image_id}.xml'\n",
        "\n",
        "            annotations,obje_num= transform_anno(path_xml)\n",
        "\n",
        "            boxes = torch.as_tensor(annotations['bboxes'], dtype=torch.int64)\n",
        "            labels = torch.as_tensor(annotations['labels'], dtype=torch.int64)\n",
        "\n",
        "\n",
        "            #bboxの縮小\n",
        "            #print('縮小前:',boxes)\n",
        "            boxes=boxes*ratio\n",
        "            #print('縮小後:',boxes)\n",
        "\n",
        "            area = (boxes[:, 3]-boxes[:, 1]) * (boxes[:, 2]-boxes[:, 0])\n",
        "            area = torch.as_tensor(area, dtype=torch.float32)\n",
        "\n",
        "            iscrowd = torch.zeros((obje_num,), dtype=torch.int64)\n",
        "\n",
        "            target = {}\n",
        "            target[\"boxes\"] = boxes\n",
        "            target[\"labels\"] = labels+1\n",
        "            target[\"image_id\"] = torch.tensor([index])\n",
        "            target[\"area\"] = area\n",
        "            target[\"iscrowd\"] = iscrowd\n",
        "            return image, target,image_id\n",
        "\n",
        "        def __len__(self):\n",
        "\n",
        "            return len(self.image_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A30hif0TreEt"
      },
      "source": [
        "#annotationの読み込み\n",
        "def dataloader (data,dataset_class,batch_size,scale=720):\n",
        "    xml_paths=data[0]\n",
        "    image_dir1=data[1]\n",
        "    dataset = MyDataset(image_dir1,xml_paths,scale,dataset_class)\n",
        "\n",
        "    #データのロード\n",
        "    torch.manual_seed(2020)\n",
        "    def collate_fn(batch):\n",
        "        return tuple(zip(*batch))\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "    return train_dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjGsX_7arjow"
      },
      "source": [
        "#モデルの定義\n",
        "\n",
        "#model: resnet50(精度が良いとされているが、mobilenet_v2と大きな差は感じない・・・)\n",
        "# def model ():\n",
        "\n",
        "    # model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "    # num_classes=len(dataset_class)+1\n",
        "    # in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    # return model\n",
        "\n",
        "    \n",
        "\n",
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "#model: mobilenet_v2(resnet50より軽量で速い)\n",
        "def model():\n",
        "\n",
        "  backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
        "  backbone.out_channels = 1280\n",
        "  anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
        "                                    aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "\n",
        "  roi_pooler =torchvision.ops.MultiScaleRoIAlign(\n",
        "                  featmap_names=['0','1','2','3'],\n",
        "                  output_size=7,\n",
        "                  sampling_ratio=2)\n",
        "\n",
        "\n",
        "  # put the pieces together inside a FasterRCNN model\n",
        "  model = FasterRCNN(backbone,\n",
        "                    num_classes=(len(dataset_class)) + 1,###注意\n",
        "                    rpn_anchor_generator=anchor_generator)\n",
        "                    #box_roi_pool=roi_pooler)\n",
        "\n",
        "  return model\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uIeDIxlrox8"
      },
      "source": [
        "#training(時間かかる部分)\n",
        "\n",
        "data_ALL=[bdd_xml,bdd_img]\n",
        "train_dataloader=dataloader(data_ALL,dataset_class,batch_size,scale)\n",
        "\n",
        "model=model()\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "num_epochs = epochs\n",
        "\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') \n",
        "\n",
        "model.cuda()\n",
        "\n",
        "model.train()#学習モードに移行\n",
        "\n",
        "loss_list=[]\n",
        "for epoch in range(num_epochs):\n",
        "    loss_epo=[]\n",
        "    \n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "\n",
        "        images, targets, image_ids = batch#####　batchはそのミニバッジのimage、tagets,image_idsが入ってる\n",
        "\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        ##学習モードでは画像とターゲット（ground-truth）を入力する\n",
        "        ##返り値はdict[tensor]でlossが入ってる。（RPNとRCNN両方のloss）\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        loss_value = losses.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        #lossの保存\n",
        "        loss_epo.append(loss_value)\n",
        "        if (i+1) % 10 == 0:\n",
        "          print(f\"epoch #{epoch+1} Iteration #{i+1} loss: {loss_value}\") \n",
        "\n",
        "    #Epochごとのlossの保存\n",
        "    loss_list.append(np.mean(loss_epo))\n",
        "    torch.save(model, path+'/***.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UY3nWCk1rsFX"
      },
      "source": [
        "#評価用画像データと学習済モデルの読み込み\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data_class=dataset_class\n",
        "data_class.insert(0, \"__background__\")\n",
        "classes = tuple(data_class)\n",
        "\n",
        "#学習済みモデルで推論する場合\n",
        "model=torch.load(path+'/***.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HN0i4xjrucb"
      },
      "source": [
        "#faster-rcnnを用いた物体検出の結果表示\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') \n",
        "\n",
        "model.to(device)\n",
        "\n",
        "model.eval()\n",
        "for imgfile in sorted(glob(test_path+'/*')):\n",
        "    img = cv2.imread(imgfile)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    image_tensor = torchvision.transforms.functional.to_tensor(img)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prediction = model([image_tensor.to(device)])\n",
        "\n",
        "    for i,box in enumerate(prediction[0]['boxes']):\n",
        "        score = prediction[0]['scores'][i].cpu().numpy()\n",
        "        if score > 0.5:\n",
        "            score = round(float(score),2)\n",
        "            cat = prediction[0]['labels'][i].cpu().numpy()\n",
        "            txt = '{} {}'.format(classes[int(cat)], str(score))\n",
        "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "            cat_size = cv2.getTextSize(txt, font, 4, 2)[0]\n",
        "            c = colors[int(cat)]\n",
        "            box=box.cpu().numpy().astype('int')\n",
        "            cv2.rectangle(img, (box[0], box[1]), (box[2], box[3]), c , 5)\n",
        "            cv2.rectangle(img,(box[0], box[1] - cat_size[1] - 2),(box[0] + cat_size[0], box[1] - 2), c, -1)\n",
        "            cv2.putText(img, txt, (box[0], box[1] - 2), font, 2, (0, 0, 0), thickness=3, lineType=cv2.LINE_AA)\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(15,10))\n",
        "    plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rL8m1zes5ift"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}